# LAMM

LAMM (pronounced as /lÃ¦m/, means cute lamb to show appreciation to LLaMA), is a growing open-source community aimed at helping researchers and developers quickly train and evaluate Multi-modal Large Language Models (MLLM), and futher build multi-modal AI agents capable of bridging the gap between ideas and execution, enabling seamless interaction between humans and AI machines.

<p align="center">
    <font size='4'>
    <a href="https://openlamm.github.io/" target="_blank">ğŸŒ Project Page</a>
    </font>
</p>

## Updates
ğŸ“† [**2023-11**] 
1. ChEF and Octavius are available!

ğŸ“† [**2023-09**]
1. LAMM is accepted by NeurIPS2023 Datasets & Benchmark Track! See you in December!'
2. Training LAMM on V100 or RTX3090 is available! Finetuning LLaMA2 is online.'
3. Our demo moved to <a href="https://openxlab.org.cn/apps/detail/LAMM/LAMM" target="_blank">OpenXLab</a>.

ğŸ“† [**2023-07**]
1.  Checkpoints & Leaderboard of LAMM on huggingface updated on new code base.
2.  Evaluation code for both 2D and 3D tasks are ready.
3.  Command line demo tools updated.

ğŸ“† [**2023-06**]
1. Watch demo video for LAMM at <a href="https://www.youtube.com/watch?v=M7XlIe8hhPk" target="_blank">YouTube</a> or <a href="https://www.bilibili.com/video/BV1kN411D7kt/" target="_blank">Bilibili</a>!
2. Full paper with Appendix is available on <a href="https://arxiv.org/abs/2306.06687" target="_blank">Arxiv</a>.
3. LAMM dataset released on <a href="https://huggingface.co/datasets/openlamm/LAMM_Dataset" target="_blank">Huggingface</a> & <a href="https://opendatalab.com/LAMM/LAMM" target="_blank">OpenDataLab</a> for Research community!',
4. LAMM code is available for Research community!


## Paper List
**Publications**

- [x] [LAMM](https://openlamm.github.io/paper_list/LAMM)


**Preprints**
- [x] [ChEF](https://openlamm.github.io/paper_list/ChEF)
- [x] [Octavius](https://openlamm.github.io/paper_list/Octavius)

## Citation
**LAMM**

```
@article{yin2023lamm,
    title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
    author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},
    journal={arXiv preprint arXiv:2306.06687},
    year={2023}
}
```

**ChEF**

```
TODO
```

**Octavius**

```
TODO
```


## Get Started
Please see [tutorial](https://openlamm.github.io/tutorial) for the basic usage of this repo.

## License 

The project is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. 
